{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN2_Notebook.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUQ3d-tdXyIz"
      },
      "source": [
        "##**Activar Google Drive en el entorno**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wztZ1h0_YRE-"
      },
      "source": [
        "Se recomienda el uso de Google Drive para el almacenamiento de datos y el entrenamiento de StyleGAN2. Al ejecutar esta celda, sigue los pasos indicados en la consola."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnrlLYbUYB09"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4iTTk22Xm-U"
      },
      "source": [
        "##**Librerías**##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2k-uPopY6cC"
      },
      "source": [
        "En la siguiente celda se importan todas las librerías necesarias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxcTLy5-XdGB"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "import os, sys\n",
        "import cv2 as cv\n",
        "from scipy import ndimage, misc\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import PIL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veF3RyEiX08D"
      },
      "source": [
        "**GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGhdl79aZBOJ"
      },
      "source": [
        "Este comando obtiene información acerca de la GPU usada para el entrenamiento y evaluación de StyleGAN2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxnDcj-GXyfU"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pcYjhauYi76"
      },
      "source": [
        "**Modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mi-wrkXZHxY"
      },
      "source": [
        "Descarga del modelo desde el GitHub de Nvidia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p7LgZdVYfJ1"
      },
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "!pip install ninja"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWFB2LGlY2nj"
      },
      "source": [
        "##**Datos de entrenamiento**##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMr9tE9QZQyj"
      },
      "source": [
        "Para entrenar a StyleGAN2 se necesita un dataset con imágenes que sean cuadradas y en modo de color RGB. Sin embargo, se puede usar cualquier dataset, ya que en esta sección se podrán corregir o eliminar las imágenes que no cumplan los requisitos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrxCY5qKh3bp"
      },
      "source": [
        "!git clone https://github.com/cardstdani/WasteClassificationNeuralNetwork.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9xDDynhY2np"
      },
      "source": [
        "**Redimensionar las imágenes de nuestro conjunto de datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2b3jyMkZsg7"
      },
      "source": [
        "Esta celda copia todas las imágenes del dataset a una carpeta que será usada durante el resto del entrenamiento. Además, redimensiona las imágenes a un tamaño estándar para todo el dataset, en este caso es (256, 256). <br><br>\n",
        "**Recuerda:** El tamaño de las imágenes puede incrementar significativamente el tiempo de entrenamiento, ya que es un modelo complejo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkj0SUFxY2nm"
      },
      "source": [
        "!mkdir dataset #Crear la carpeta donde se guardará el conjunto de datos\n",
        "\n",
        "outPath = \"/content/dataset\" #Ruta de salida\n",
        "path = \"/content/WasteClassificationNeuralNetwork/WasteImagesDataset\" #Ruta donde se encuentra nuestro dataset (ES NECESARIO COMPROBAR ESTA RUTA)\n",
        "dim = (256, 256) #Dimensiones de salida para todas las imágenes del dataset\n",
        "\n",
        "for p in os.scandir(path):\n",
        "  if p.is_dir():\n",
        "    for image_path in os.listdir(p):\n",
        "      image_to_modify = imageio.imread(os.path.join(p, image_path))\n",
        "      changed = np.array(Image.fromarray(image_to_modify).resize(dim, Image.BILINEAR)).astype(np.double)\n",
        "      fullpath = os.path.join(outPath, 'out_'+image_path)\n",
        "      imageio.imwrite(fullpath, changed)\n",
        "  else:\n",
        "    image_to_modify = imageio.imread(p.path)\n",
        "    changed = np.array(Image.fromarray(image_to_modify).resize(dim, Image.BILINEAR)).astype(np.double)\n",
        "    fullpath = os.path.join(outPath, 'out_'+p.name)\n",
        "    imageio.imwrite(fullpath, changed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0atL6xTY2ns"
      },
      "source": [
        "**Comprobación de las imágenes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8fCQWquEHId"
      },
      "source": [
        "Este paso es uno de los más importantes. Se comprueba todas las imágenes del dataset, y todas las que no sean cuadradas o estén en modo de color RGB son eliminadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dozv6EwwY2nu"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import join\n",
        "\n",
        "imgNumber = 0\n",
        "base_size = None\n",
        "for file in os.listdir(outPath):\n",
        "  imgNumber += 1\n",
        "  file2 = os.path.join(outPath,file)\n",
        "  img = Image.open(file2)\n",
        "  sz = img.size\n",
        "  \n",
        "  #Eliminar todas las imágenes erróneas\n",
        "  if base_size and sz!=base_size:\n",
        "    print(f\"Tamaño inconsistente: {file2}\")\n",
        "    os.remove(file2)\n",
        "    imgNumber -= 1\n",
        "  elif img.mode!='RGB':\n",
        "    print(f\"El formato de color no es RGB: {file2}\")\n",
        "    os.remove(file2)\n",
        "    imgNumber -= 1\n",
        "  else:\n",
        "    base_size = sz\n",
        "\n",
        "print(\"El dataset contiene: \" + str(imgNumber) + \" imágenes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBAdKiIyadxC"
      },
      "source": [
        "##**Entrenamiento**##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG7EgN69a5SD"
      },
      "source": [
        "En esta sección se entrena el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdKfhitDbEZ1"
      },
      "source": [
        "outputDir = \"/content/drive/MyDrive/StyleGAN2/WasteClassification/\" #carpeta en la que se guardan los resultados del entrenamiento (se recomienda guardar en drive)\n",
        "\n",
        "snapshot_count = 1 #Variable de autoguardado, es decir, cada n iteraciones se guarda el modelo y los resultados en la carpeta de salida\n",
        "mirrored = True #Reflejar las imágenes horizontalmente (True/False)\n",
        "metric_list = None #Lista de métricas, por defecto en None\n",
        "numGPUS = 1 #Número de tarjetas gráficas usadas para el entrenamiento, aquí en Google Colab por defecto está en 1\n",
        "augs = \"bg\" #Canal de aumento de las imágenes, por defecto en \"bg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eWv1sRZadxD"
      },
      "source": [
        "**Para empezar desde 0 el entrenamiento**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "titPih2mbFui"
      },
      "source": [
        "Esta celda debe usarse solo cuando se comienza a entrenar un modelo desde 0, si ya tienes un modelo preentrenado y quieres continuar, ve a la celda siguiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y8hkT0dadxE"
      },
      "source": [
        "!python /content/stylegan2-ada-pytorch/train.py --outdir {outputDir} --gpus={numGPUS} --snap={snapshot_count} --data={outPath} --augpipe={augs} --mirror={mirrored} --metrics={metric_list}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPUwRVcYadxF"
      },
      "source": [
        "**Para continuar con un entrenamiento previo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huHHEpIdctAs"
      },
      "source": [
        "Los modelos generados por StyleGAN2 se guardan en formato .pkl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUpRevdEadxG"
      },
      "source": [
        "resume_from = \"/content/drive/MyDrive/StyleGAN2/WasteClassification/00006-dataset-mirror-auto1-bg/network-snapshot-000004.pkl\" #Ruta del modelo con el que se reanudará el entrenamiento (ES NECESARIO COMPROBAR ESTA RUTA)\n",
        "\n",
        "!python /content/stylegan2-ada-pytorch/train.py --outdir {outputDir} --gpus={numGPUS} --snap={snapshot_count} --data={outPath} --augpipe={augs} --mirror={mirrored} --metrics={metric_list} --resume={resume_from}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AWXQ2MGd-Jw"
      },
      "source": [
        "##**Resultados**##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLw3gnNxgwZH"
      },
      "source": [
        "**Cargar modelo y funciones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWgWUqMBe_EH"
      },
      "source": [
        "sys.path.insert(0, \"/content/stylegan2-ada-pytorch\")\n",
        "import pickle\n",
        "from IPython.display import Image\n",
        "import IPython.display\n",
        "import dnnlib\n",
        "!pip install legacy\n",
        "import legacy\n",
        "\n",
        "def seed2vec(G, seed):\n",
        "  return np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "\n",
        "def get_label(G, device, class_idx):\n",
        "  label = torch.zeros([1, G.c_dim], device=device)\n",
        "  if G.c_dim != 0:\n",
        "      if class_idx is None:\n",
        "          ctx.fail('Must specify class label with --class when using a conditional network')\n",
        "      label[:, class_idx] = 1\n",
        "  else:\n",
        "      if class_idx is not None:\n",
        "          print ('warn: --class=lbl ignored when running on an unconditional network')\n",
        "  return label\n",
        "\n",
        "def generate_image(device, G, z, truncation_psi=1.0, noise_mode='const', class_idx=None):\n",
        "  z = torch.from_numpy(z).to(device)\n",
        "  label = get_label(G, device, class_idx)\n",
        "  img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "  img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "  return PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "device = torch.device('cuda')\n",
        "modelPath = \"/content/drive/MyDrive/StyleGAN2/WasteClassification/00004-dataset-mirror-auto1-bg-resumecustom/network-snapshot-000176.pkl\" #Ruta del modelo que vamos a probar (ES NECESARIO COMPROBAR ESTA RUTA)\n",
        "with open(modelPath, 'rb') as f:\n",
        "    G = legacy.load_network_pkl(f)['G_ema'].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOZOQbCcd-Jy"
      },
      "source": [
        "**Generar imágenes en un rango de \"semillas\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO5BNWd3blWk"
      },
      "source": [
        "Las semillas son números que hacen la función de \"identificadores\" para todas las imágenes que puede generar el modelo. Esta celda se encarga de generar y mostrar todas las imágenes que existen entre la \"semilla\" de inicio y final."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8mhLOvad-Jx"
      },
      "source": [
        "SEED_FROM = 1000 #Semilla de inicio\n",
        "SEED_TO = 1002 #Semilla final\n",
        "\n",
        "# Se muestran todas las imágenes que el modelo genera entre los valores de la semilla de inicio y final\n",
        "for i in range(SEED_FROM, SEED_TO):\n",
        "  print(\"Seed: \" + str(i))\n",
        "  z = seed2vec(G, i)\n",
        "  img = generate_image(device, G, z)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP_H4lTdi3FW"
      },
      "source": [
        "**Generar un vídeo con una interpolación de imágenes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv1KwIA8cVat"
      },
      "source": [
        "En este caso, en vez de simplemente generar y mostrar las imágenes, se crea un vídeo con la interpolación de las imágenes generadas y se guarda en el entorno."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzV-THzAjBL-"
      },
      "source": [
        "def expand_seed(seeds, vector_size):\n",
        "  result = []\n",
        "\n",
        "  for seed in seeds:\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    result.append( rnd.randn(1, vector_size) ) \n",
        "  return result\n",
        "\n",
        "vector_size = G.z_dim\n",
        "\n",
        "SEEDS = [10, 15, 12] #Configura el número de semillas (comienzo, semilla intermedia, final)\n",
        "STEPS = 100 #Número de pasos por cada semilla\n",
        "\n",
        "# Borrar resultados previos (si hay alguno)\n",
        "!rm /content/resultados/* \n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "os.makedirs(\"./resultados/\", exist_ok=True) #Crear una carpeta llamada resultados donde se almacenarán los frames del vídeo\n",
        "\n",
        "# Generar y guardar en la carpeta \"resultados\" todos los frames del vídeo\n",
        "idx = 0\n",
        "for i in range(len(SEEDS)-1):\n",
        "  v1 = seed2vec(G, SEEDS[i])\n",
        "  v2 = seed2vec(G, SEEDS[i+1])\n",
        "\n",
        "  step = (v2 - v1) / STEPS\n",
        "  current = v1.copy()\n",
        "\n",
        "  for j in tqdm(range(STEPS), desc=f\"Seed {SEEDS[i]}\"):\n",
        "    current = current + step\n",
        "    img = generate_image(device, G, current)\n",
        "    img.save(f'./resultados/frame-{idx}.png')\n",
        "    idx+=1\n",
        " \n",
        "outputVideoName = \"output_video\" #Nombre del vídeo de salida\n",
        "!ffmpeg -r 30 -i /content/resultados/frame-%d.png -vcodec mpeg4 -y {outputVideoName}.mp4"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}